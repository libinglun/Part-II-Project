{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:11:38.494468Z",
     "start_time": "2024-03-25T12:11:34.841759Z"
    }
   },
   "id": "e79e8de068d66b80"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:11:38.505165Z",
     "start_time": "2024-03-25T12:11:38.498869Z"
    }
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \n",
    "                            \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                    \n",
    "                            \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tagger = PerceptronTagger()\n",
    "tagset = 'universal'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:11:38.552781Z",
     "start_time": "2024-03-25T12:11:38.502776Z"
    }
   },
   "id": "8ec35df02f58e742"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 'DET'),\n ('mat', 'NOUN'),\n ('sat', 'VERB'),\n ('on', 'ADP'),\n ('the', 'DET'),\n ('cat', 'NOUN')]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize('the mat sat on the cat')\n",
    "tags = nltk.tag._pos_tag(tokens, tagset, tagger, lang='eng')\n",
    "tags"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:11:38.564612Z",
     "start_time": "2024-03-25T12:11:38.554286Z"
    }
   },
   "id": "81a18bb4745749c0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"(\\w) '(\\w)\", r\"\\1'\\2\", text)  # remove the space before the apostrophe\n",
    "    text = re.sub(r\" n't\", \"n't\", text)\n",
    "    # print(text)\n",
    "    text = re.sub(r'\\(.*\\)', '', text)            # remove word in parentheses\n",
    "    \n",
    "    \n",
    "    words = text.split(' ')\n",
    "    result = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in contraction_mapping: \n",
    "            result += contraction_mapping[words[i]].split(' ')\n",
    "            continue\n",
    "        result.append(words[i])\n",
    "            \n",
    "    text = ' '.join(result)\n",
    "    text = text.replace(\"'s\",'')\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]','',text)      # remove punctuations\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:11:41.107940Z",
     "start_time": "2024-03-25T12:11:41.092511Z"
    }
   },
   "id": "5308a1817ba8d625"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'you take paul turn because he cannot do that but i will do that since it is a nice day'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"you take paul 's turn because he ca n't do that but I 'll do that since it 's a nice day.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:11:42.147493Z",
     "start_time": "2024-03-25T12:11:42.131080Z"
    }
   },
   "id": "2ed470e66faaa8d4"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"../../../data/all_sents_shuffled.raw\", 'r') as file: \n",
    "    for line in file: \n",
    "        line = line.strip()\n",
    "        sentences.append(preprocess(line)) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:13:41.730036Z",
     "start_time": "2024-03-25T12:13:40.778434Z"
    }
   },
   "id": "528a30079839bb26"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(['you stepped on my calf and that hurt',\n  'is not he cute',\n  'they are playing in your room',\n  'clucko clucko clucko do you live up in the mountains',\n  'we can play a couple games'],\n 223675)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[100:105], len(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:14:16.603334Z",
     "start_time": "2024-03-25T12:14:16.600163Z"
    }
   },
   "id": "12f70762c19e9f38"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "tokenized_dataset = []\n",
    "for sent in sentences: \n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    tags = nltk.tag._pos_tag(tokens, tagset, tagger, lang='eng')\n",
    "    tokenized_dataset.append(tags)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:14:08.086749Z",
     "start_time": "2024-03-25T12:13:44.380406Z"
    }
   },
   "id": "b90e52394f1049d7"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[[('you', 'PRON'),\n  ('take', 'VERB'),\n  ('paul', 'ADJ'),\n  ('turn', 'NOUN'),\n  ('because', 'ADP'),\n  ('he', 'PRON'),\n  ('can', 'VERB'),\n  ('not', 'ADV'),\n  ('do', 'VERB'),\n  ('that', 'ADP')],\n [('you', 'PRON')],\n [('my', 'PRON'),\n  ('this', 'DET'),\n  ('is', 'VERB'),\n  ('a', 'DET'),\n  ('big', 'ADJ'),\n  ('bed', 'NOUN')],\n [('yes', 'NOUN')],\n [('huh', 'NOUN')]]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:15:13.680746Z",
     "start_time": "2024-01-22T10:15:13.662212Z"
    }
   },
   "id": "7b1ed0296c357df"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "def count_length(dataset, length):\n",
    "    count = 0\n",
    "    for sent in dataset: \n",
    "        if len(sent) == length: \n",
    "            count += 1\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T21:33:49.171148Z",
     "start_time": "2024-01-18T21:33:49.139528Z"
    }
   },
   "id": "5c74871026928710"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "(46602, 22274, 30116, 223675)"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_length(tokenized_dataset, 1), count_length(tokenized_dataset, 2), count_length(tokenized_dataset, 3), len(tokenized_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T21:33:50.662449Z",
     "start_time": "2024-01-18T21:33:50.660248Z"
    }
   },
   "id": "91fe4fd9b2092210"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def filter_length(dataset, filter_len=7):\n",
    "    return [sent for sent in dataset if len(sent) >= filter_len]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:48:42.956831Z",
     "start_time": "2024-01-22T10:48:42.840282Z"
    }
   },
   "id": "9a8bc8899ee44153"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "47246"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset = filter_length(tokenized_dataset)\n",
    "len(filtered_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:48:43.932578Z",
     "start_time": "2024-01-22T10:48:43.909006Z"
    }
   },
   "id": "92c79b75c9ea7012"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def replace_low_frequency_words(sentences_with_pos_tags, filter_count=1):\n",
    "    # Count the frequencies of each word\n",
    "    word_counts = Counter(word for sentence in sentences_with_pos_tags for word, _ in sentence)\n",
    "\n",
    "    # Replace words with count less than filter_count to 'UNK' and their tags to 'UNK_TAG'\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences_with_pos_tags:\n",
    "        new_sentence = []\n",
    "        for word, pos in sentence:\n",
    "            if pos in ['X', '.']:\n",
    "                continue\n",
    "            if word_counts[word] < filter_count:\n",
    "                new_word = 'UNK'                # Only set the word to UNK\n",
    "                new_pos = pos\n",
    "            else:\n",
    "                new_word = word\n",
    "                new_pos = pos\n",
    "            new_sentence.append((new_word, new_pos))\n",
    "        processed_sentences.append(new_sentence)\n",
    "\n",
    "    return processed_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:43:16.401718Z",
     "start_time": "2024-01-22T10:43:16.383860Z"
    }
   },
   "id": "315c4ad9e812357e"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "[[('you', 'PRON'),\n  ('take', 'VERB'),\n  ('paul', 'ADJ'),\n  ('turn', 'NOUN'),\n  ('because', 'ADP'),\n  ('he', 'PRON'),\n  ('can', 'VERB'),\n  ('not', 'ADV'),\n  ('do', 'VERB'),\n  ('that', 'ADP')],\n [('let', 'VERB'),\n  ('us', 'PRON'),\n  ('get', 'VERB'),\n  ('out', 'ADP'),\n  ('of', 'ADP'),\n  ('the', 'DET'),\n  ('bathroom', 'NOUN')],\n [('when', 'ADV'),\n  ('nana', 'NOUN'),\n  ('comes', 'VERB'),\n  ('down', 'ADV'),\n  ('to', 'PRT'),\n  ('the', 'DET'),\n  ('UNK', 'NOUN'),\n  ('office', 'NOUN'),\n  ('and', 'CONJ'),\n  ('get', 'VERB'),\n  ('it', 'PRON')]]"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentences = replace_low_frequency_words(filtered_dataset, filter_count=10)\n",
    "filtered_sentences[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:48:51.174952Z",
     "start_time": "2024-01-22T10:48:51.057863Z"
    }
   },
   "id": "8ed925f65beb132d"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def create_vocab_index(sentences_with_pos_tags):\n",
    "    # Function to create a dictionary mapping each unique word/POS to an integer index\n",
    "    # with specified start index\n",
    "    def build_index(items, start_index=0):\n",
    "        item_to_index = defaultdict(lambda: len(item_to_index) + start_index)\n",
    "        for item in items:\n",
    "            item_to_index[item]\n",
    "        return dict(item_to_index)\n",
    "\n",
    "    all_words = [word for sentence in sentences_with_pos_tags for word,_ in sentence]\n",
    "    all_pos_tags = [pos for sentence in sentences_with_pos_tags for _, pos in sentence]\n",
    "\n",
    "    return build_index(all_words, start_index=0), build_index(all_pos_tags, start_index=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:43:19.094388Z",
     "start_time": "2024-01-22T10:43:19.077421Z"
    }
   },
   "id": "31af4c2292273cec"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n"
     ]
    },
    {
     "data": {
      "text/plain": "([('let', 10), ('us', 11), ('get', 12), ('out', 13), ('of', 14)],\n [('PRON', 1),\n  ('VERB', 2),\n  ('ADJ', 3),\n  ('NOUN', 4),\n  ('ADP', 5),\n  ('ADV', 6),\n  ('DET', 7),\n  ('PRT', 8),\n  ('CONJ', 9),\n  ('NUM', 10)])"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index, pos_to_index = create_vocab_index(filtered_sentences)\n",
    "print(len(word_to_index))\n",
    "list(word_to_index.items())[10:15], list(pos_to_index.items())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:49:14.206048Z",
     "start_time": "2024-01-22T10:49:14.125829Z"
    }
   },
   "id": "cbb19396bbad2dac"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def count_tags(tag, sentences_with_pos_tags):\n",
    "    count = 0\n",
    "    for sentence in sentences_with_pos_tags: \n",
    "        for _, pos in sentence:\n",
    "            if pos == tag: \n",
    "                print(sentence)\n",
    "                count += 1\n",
    "                \n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:15:52.304606Z",
     "start_time": "2024-01-22T10:15:52.261294Z"
    }
   },
   "id": "b272e4dfd22865f4"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tags(\".\", filtered_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:15:53.581465Z",
     "start_time": "2024-01-22T10:15:53.495207Z"
    }
   },
   "id": "1408091c8835e76a"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def convert_to_indexes(filtered_sentences_tags, word_to_index, pos_to_index):\n",
    "    hidden_states_universal = []\n",
    "    observations = []\n",
    "\n",
    "    for sentence in filtered_sentences_tags:\n",
    "        # if len(sentence) <= 5: \n",
    "        #     continue\n",
    "        sentence_pos_indexes = [pos_to_index[pos] for _, pos in sentence]\n",
    "        sentence_word_indexes = [word_to_index[word] for word, _ in sentence]\n",
    "\n",
    "        hidden_states_universal.append(sentence_pos_indexes)\n",
    "        observations.append(sentence_word_indexes)\n",
    "\n",
    "    return hidden_states_universal, observations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:43:30.298155Z",
     "start_time": "2024-01-22T10:43:30.281903Z"
    }
   },
   "id": "7dc30154e6297035"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "hidden_states_universal, observations = convert_to_indexes(\n",
    "    filtered_sentences, word_to_index, pos_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:49:47.502101Z",
     "start_time": "2024-01-22T10:49:47.462467Z"
    }
   },
   "id": "f1acc18f254a6a7b"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 1, 2, 6, 2, 5]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-----------------------------\n",
      "[1, 7, 2, 7, 3, 4]\n",
      "[10, 11, 12, 13, 14, 15]\n",
      "-----------------------------\n",
      "[2, 1, 2, 5, 5, 7, 4]\n",
      "[16, 17, 18, 19, 20, 21, 22]\n",
      "-----------------------------\n",
      "[3, 4, 2, 8, 4]\n",
      "[14, 23, 24, 25, 26]\n",
      "-----------------------------\n",
      "[9, 3, 2, 6, 3]\n",
      "[27, 28, 29, 30, 31]\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(observations[:5])):\n",
    "    print('[' + ', '.join(map(str, hidden_states_universal[:5][index])) + ']')\n",
    "    print('[' + ', '.join(map(str, observations[:5][index])) + ']')\n",
    "    print('-----------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:43:33.175102Z",
     "start_time": "2024-01-22T10:43:33.157716Z"
    }
   },
   "id": "d1ef8ea4a5c28abb"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def add_noise_to_states_childes(hidden_states, number_states, flip_prob=0.3):\n",
    "    noisy_hidden_states = []\n",
    "    for sequence in hidden_states:\n",
    "        noisy_sequence = []\n",
    "        for state in sequence:\n",
    "            if np.random.rand() < flip_prob:\n",
    "                possible_states = list(range(1, number_states + 1))  # Flip the state to a different random state\n",
    "                possible_states.remove(state)                        # Remove the current state from possibilities\n",
    "                new_state = np.random.choice(possible_states)\n",
    "                noisy_sequence.append(new_state)\n",
    "            else:\n",
    "                noisy_sequence.append(state)\n",
    "        noisy_hidden_states.append(noisy_sequence)\n",
    "    return noisy_hidden_states"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:43:36.031962Z",
     "start_time": "2024-01-22T10:43:36.015066Z"
    }
   },
   "id": "104ac74afa625e0b"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "noisy_level = 0.8\n",
    "noisy_hidden_states_universal = add_noise_to_states_childes(hidden_states_universal, len(pos_to_index), flip_prob=noisy_level)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:49:53.119084Z",
     "start_time": "2024-01-22T10:49:51.548020Z"
    }
   },
   "id": "346800e60a9a641"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "for i in range(len(hidden_states_universal)):\n",
    "    hidden_states_universal[i].insert(0, 0)\n",
    "    noisy_hidden_states_universal[i].insert(0, 0)\n",
    "    \n",
    "    observations[i].insert(0, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:49:53.903628Z",
     "start_time": "2024-01-22T10:49:53.899052Z"
    }
   },
   "id": "36251ca859676c09"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 2, 6, 7, 3, 3, 4, 2, 2, 1, 5]\n",
      "[0, 8, 9, 3, 1, 3, 6, 3, 7, 10, 5, 5]\n",
      "-----------------------------\n",
      "[0, 5, 1, 2, 8, 2, 1, 2, 2, 1, 2, 6, 3]\n",
      "[0, 6, 6, 5, 6, 2, 4, 5, 9, 1, 6, 6, 4]\n",
      "-----------------------------\n",
      "[0, 7, 4, 7, 2, 2, 6, 6, 1, 6, 2, 5, 1, 6, 1]\n",
      "[0, 9, 4, 6, 3, 2, 9, 2, 9, 9, 2, 5, 4, 9, 6]\n",
      "-----------------------------\n",
      "[0, 3, 6, 2, 6, 1, 2, 8, 6]\n",
      "[0, 10, 6, 10, 3, 5, 2, 5, 6]\n",
      "-----------------------------\n",
      "[0, 1, 6, 2, 1, 5, 7, 4, 7, 5, 1, 2, 1, 8, 9, 6, 2, 1, 5, 5, 10, 4]\n",
      "[0, 7, 6, 2, 8, 5, 6, 4, 7, 3, 8, 5, 8, 2, 4, 6, 4, 10, 5, 4, 2, 4]\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(observations[10030:10035])):\n",
    "    print('[' + ', '.join(map(str, hidden_states_universal[10030:10035][index])) + ']')\n",
    "    print('[' + ', '.join(map(str, noisy_hidden_states_universal[10030:10035][index])) + ']')\n",
    "    print('-----------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:49:55.833352Z",
     "start_time": "2024-01-22T10:49:55.815457Z"
    }
   },
   "id": "9a25f7aee66f3a10"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "file_path = f\"../data/Childes_synthetic_dataset(noise-{noisy_level}).npz\"\n",
    "obs_object = np.array(observations, dtype=object)\n",
    "uni_hid_object = np.array(hidden_states_universal, dtype=object)\n",
    "noisy_uni_hid_object = np.array(noisy_hidden_states_universal, dtype=object)\n",
    "np.savez(file_path, num_states=len(pos_to_index) + 1, num_obs=len(word_to_index), observation=obs_object, real_hidden_universal=uni_hid_object, noisy_hidden_universal=noisy_uni_hid_object, noisy_level=noisy_level)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:49:58.574440Z",
     "start_time": "2024-01-22T10:49:58.165363Z"
    }
   },
   "id": "e5054a99dc5fabcd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "projenv",
   "language": "python",
   "display_name": "Part II Project Environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
